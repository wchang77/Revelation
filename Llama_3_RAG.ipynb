{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8520d840-fcf6-4458-b85c-8a2ff80a34eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8520d840-fcf6-4458-b85c-8a2ff80a34eb",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "7c9e4102-3d9c-478d-d9a6-710c2c7c5734"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python gpt4all pypdf colab-xterm gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ljlKnZzKjFNy",
      "metadata": {
        "id": "ljlKnZzKjFNy"
      },
      "source": [
        "# Note:\n",
        "\n",
        "### Type in your terminal:\n",
        "\n",
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "ollama serve & ollama pull llama3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2096d49c-d3dc-4329-ada7-aff56d210198",
      "metadata": {
        "id": "2096d49c-d3dc-4329-ada7-aff56d210198"
      },
      "outputs": [],
      "source": [
        "### LLM\n",
        "\n",
        "local_llm = \"llama3\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e091a110",
      "metadata": {
        "id": "e091a110"
      },
      "source": [
        "# URL Type Router"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "335dda13",
      "metadata": {
        "id": "335dda13"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def url_type(url):\n",
        "    \"\"\"\n",
        "    Determine the type of the URL: PDF or webpage.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL to check.\n",
        "\n",
        "    Returns:\n",
        "        str: The type of the URL: 'pdf' or 'webpage'.\n",
        "    \"\"\"\n",
        "    # Check if the URL ends with .pdf\n",
        "    if url.lower().endswith('.pdf'):\n",
        "        return 'pdf'\n",
        "\n",
        "    # Check the Content-Type and Content-Disposition headers\n",
        "    try:\n",
        "        response = requests.head(url, allow_redirects=True)\n",
        "        content_type = response.headers.get('Content-Type', '').lower()\n",
        "        content_disposition = response.headers.get('Content-Disposition', '').lower()\n",
        "\n",
        "        # Check if Content-Type indicates a PDF\n",
        "        if 'application/pdf' in content_type:\n",
        "            return 'pdf'\n",
        "\n",
        "        # Check if Content-Disposition indicates a PDF filename\n",
        "        if 'filename' in content_disposition and content_disposition.endswith('.pdf'):\n",
        "            return 'pdf'\n",
        "\n",
        "        # Check if Content-Type indicates an HTML page\n",
        "        if 'text/html' in content_type:\n",
        "            return 'webpage'\n",
        "    except requests.RequestException:\n",
        "        return 'unknown'\n",
        "\n",
        "    return 'unknown'\n",
        "\n",
        "# Example URL to test\n",
        "url = \"https://arxiv.org/pdf/2403.14403\"\n",
        "print(f\"The URL '{url}' is identified as a '{url_type(url)}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e839383",
      "metadata": {
        "id": "1e839383"
      },
      "source": [
        "# Document Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb1b3957",
      "metadata": {
        "id": "cb1b3957"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata\n",
        "\n",
        "    @staticmethod\n",
        "    def fetch_html(url):\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            return response.text\n",
        "        except requests.exceptions.HTTPError as err:\n",
        "            print(f\"HTTPError for URL {url}: {err}\")\n",
        "            return \"\"\n",
        "        except Exception as err:\n",
        "            print(f\"Error for URL {url}: {err}\")\n",
        "            return \"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_documents(urls):\n",
        "        documents = []\n",
        "        for url in urls:\n",
        "            html = Document.fetch_html(url)\n",
        "            documents.append(Document(page_content=html, metadata={\"source\": url}))\n",
        "            print(f\"Loaded document from URL: {url}\")  # Debugging statement\n",
        "        return documents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SzDf9Mt7bpVZ",
      "metadata": {
        "id": "SzDf9Mt7bpVZ"
      },
      "source": [
        "# Chunk & Embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o6hwtGHLeiyv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6hwtGHLeiyv",
        "outputId": "5b0b7f0a-355e-4dd0-b0b4-62c364b9f4fa"
      },
      "outputs": [],
      "source": [
        "!pip install -q html2text faiss-cpu sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7755dda8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.document_transformers import Html2TextTransformer\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def chunk_embed_documents(urls):\n",
        "    pdf_docs_list = []\n",
        "    html_docs_list = []\n",
        "\n",
        "    for url in urls:\n",
        "        if url_type(url) == 'pdf':\n",
        "            docs = PyPDFLoader(url).load()\n",
        "            pdf_docs_list.extend(docs)\n",
        "        else:\n",
        "            html_content = Document.fetch_html(url)\n",
        "            if html_content:\n",
        "                doc = Document(page_content=html_content, metadata={\"source\": url})\n",
        "                html_docs_list.append(doc)\n",
        "            else:\n",
        "                print(f\"Failed to load document from URL: {url}\")\n",
        "\n",
        "    if pdf_docs_list:\n",
        "        print(f\"Number of PDF documents loaded: {len(pdf_docs_list)}\")\n",
        "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250, chunk_overlap=0)\n",
        "        doc_splits = text_splitter.split_documents(pdf_docs_list)\n",
        "        vectorstore = FAISS.from_documents(doc_splits, HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
        "        retriever = vectorstore.as_retriever()\n",
        "        print(f\"PDF Retriever created: {retriever}\")\n",
        "        return retriever\n",
        "\n",
        "    if html_docs_list:\n",
        "        html2text = Html2TextTransformer()\n",
        "        docs_trans = html2text.transform_documents(html_docs_list)\n",
        "        print(f\"Number of HTML documents loaded: {len(docs_trans)}\")\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "        chunked_documents = text_splitter.split_documents(docs_trans)\n",
        "        db = FAISS.from_documents(chunked_documents, HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
        "        retriever = db.as_retriever()\n",
        "        print(f\"HTML Retriever created: {retriever}\")\n",
        "        return retriever\n",
        "\n",
        "    print(\"No documents to process.\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8265c1cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_pdf(urls):\n",
        "    docs = [PyPDFLoader(url).load() for url in urls]\n",
        "    docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=250, chunk_overlap=0\n",
        "    )\n",
        "    doc_splits = text_splitter.split_documents(docs_list)\n",
        "    return doc_splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "233ef267",
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_splits = load_pdf([\"https://arxiv.org/pdf/2403.14403\",\n",
        "    \"https://arxiv.org/pdf/2401.15884\",\n",
        "    \"https://arxiv.org/pdf/2310.11511\"])\n",
        "# Print each document content on a new line\n",
        "for i, doc in enumerate(doc_splits):\n",
        "    print(f\"Document {i+1}:\")\n",
        "    print(doc.page_content)\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lSxGBPS3SVzn",
      "metadata": {
        "id": "lSxGBPS3SVzn"
      },
      "source": [
        "# Agents\n",
        "\n",
        "### Make sure to enter in terminal `ollama pull llama3` before executing agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b008df98-8394-49da-8fb8-aefe2c90d03c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "b008df98-8394-49da-8fb8-aefe2c90d03c",
        "outputId": "283a559f-a95b-4f92-f84b-65e45d753030"
      },
      "outputs": [],
      "source": [
        "### Retrieval Grader\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# LLM\n",
        "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance\n",
        "    of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
        "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
        "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
        "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
        "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "    \"\"\",\n",
        "    input_variables=[\"question\", \"document\"],\n",
        ")\n",
        "\n",
        "retrieval_grader = prompt | llm | JsonOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d531a81-6d4d-405e-975a-01ef1c9679fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "1d531a81-6d4d-405e-975a-01ef1c9679fa",
        "outputId": "078ea891-3498-4a4f-87e9-5a19ec340189"
      },
      "outputs": [],
      "source": [
        "### Generate\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks.\n",
        "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
        "    Keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "    Question: {question}\n",
        "    Context: {context}\n",
        "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
        "    input_variables=[\"question\", \"document\"],\n",
        ")\n",
        "\n",
        "llm = ChatOllama(model=local_llm, temperature=0)\n",
        "\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0261a9a4-de13-4dd8-b082-95305a3e43ca",
      "metadata": {
        "id": "0261a9a4-de13-4dd8-b082-95305a3e43ca"
      },
      "outputs": [],
      "source": [
        "### Hallucination Grader\n",
        "\n",
        "# LLM\n",
        "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
        "\n",
        "# Prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether\n",
        "    an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate\n",
        "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a\n",
        "    single key 'score' and no preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "    Here are the facts:\n",
        "    \\n ------- \\n\n",
        "    {documents}\n",
        "    \\n ------- \\n\n",
        "    Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
        "    input_variables=[\"generation\", \"documents\"],\n",
        ")\n",
        "\n",
        "hallucination_grader = prompt | llm | JsonOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df9f6944-4fee-4971-b3a7-2b81b44ed433",
      "metadata": {
        "id": "df9f6944-4fee-4971-b3a7-2b81b44ed433"
      },
      "outputs": [],
      "source": [
        "### Answer Grader\n",
        "\n",
        "# LLM\n",
        "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
        "\n",
        "# Prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an\n",
        "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is\n",
        "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
        "     <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n",
        "    \\n ------- \\n\n",
        "    {generation}\n",
        "    \\n ------- \\n\n",
        "    Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
        "    input_variables=[\"generation\", \"question\"],\n",
        ")\n",
        "\n",
        "answer_grader = prompt | llm | JsonOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9c910c1-738c-4bf7-bf9e-801862b227eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "a9c910c1-738c-4bf7-bf9e-801862b227eb",
        "outputId": "dfd73192-4f78-4a22-8ad8-9ac7990e997c"
      },
      "outputs": [],
      "source": [
        "### Router\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# LLM\n",
        "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a\n",
        "    user question to a vectorstore or web search. Use the vectorstore for questions with keywords related to topics in\n",
        "    the vectorstore. You do not need to be stringent with the keywords\n",
        "    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search'\n",
        "    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and\n",
        "    no premable or explanation. Question to route: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
        "    input_variables=[\"question\"],\n",
        ")\n",
        "\n",
        "question_router = prompt | llm | JsonOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5uO_41aeU003",
      "metadata": {
        "id": "5uO_41aeU003"
      },
      "source": [
        "# Web Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "023ff2db-eb4e-4d44-904c-ea061abc16d9",
      "metadata": {
        "id": "023ff2db-eb4e-4d44-904c-ea061abc16d9"
      },
      "outputs": [],
      "source": [
        "### Search\n",
        "\n",
        "import os\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "# Set your Tavily API key\n",
        "os.environ['TAVILY_API_KEY'] = 'YOUR_API'\n",
        "\n",
        "# Initialize the TavilySearchResults tool\n",
        "web_search_tool = TavilySearchResults(k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "McGJQ5PsU6zG",
      "metadata": {
        "id": "McGJQ5PsU6zG"
      },
      "source": [
        "# LangGraph Control Flow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccd59cdf-a04d-4b2e-b9cc-6a1b1e80a6c6",
      "metadata": {
        "id": "ccd59cdf-a04d-4b2e-b9cc-6a1b1e80a6c6"
      },
      "source": [
        "### We'll implement these as a control flow in LangGraph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07fa3d08-6a86-4705-a28b-e2721070bc5e",
      "metadata": {
        "id": "07fa3d08-6a86-4705-a28b-e2721070bc5e"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "### State\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        web_search: whether to add search\n",
        "        documents: list of documents\n",
        "        urls: list of URLs to process\n",
        "    \"\"\"\n",
        "    question: str\n",
        "    generation: str\n",
        "    web_search: str\n",
        "    documents: List[str]\n",
        "    urls: List[str]  # Add this line\n",
        "    retrievers: List[Any]  # Added retrievers to the TypedDict\n",
        "\n",
        "\n",
        "def process_urls_node(state):\n",
        "    \"\"\"\n",
        "    Process URLs and update the state with retrievers.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updated state with retrievers\n",
        "    \"\"\"\n",
        "    urls = state.get(\"urls\", [])\n",
        "    print(f\"Processing URLs: {urls}\")  # Debugging statement\n",
        "\n",
        "    retrievers = state.get(\"retrievers\", [])\n",
        "\n",
        "    if urls:\n",
        "        retriever = chunk_embed_documents(urls)\n",
        "        if retriever:\n",
        "            retrievers.append(retriever)\n",
        "            print(f\"Retriever added\")  # Debugging statement\n",
        "\n",
        "    state[\"retrievers\"] = retrievers  # Store retrievers in the state\n",
        "    print(f\"Retrievers in state: {state['retrievers']}\")  # Debugging statement\n",
        "    return state\n",
        "\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents from vectorstore\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    print(f\"Retrievers in state: {state['retrievers']}\")  # Debugging statement\n",
        "    question = state[\"question\"]\n",
        "    retrievers = state.get(\"retrievers\", [])\n",
        "    print(f\"Number of retrievers: {len(retrievers)}\")  # Debugging statement\n",
        "    print(retrievers)\n",
        "    # Combine results from all retrievers\n",
        "    documents = []\n",
        "\n",
        "    for retriever in retrievers:\n",
        "        retrieved_docs = retriever.get_relevant_documents(question)\n",
        "        print(f\"Documents retrieved: {retrieved_docs}\")  # Debugging statement\n",
        "        documents.extend(retrieved_docs)\n",
        "\n",
        "    print(f\"Total number of documents retrieved: {len(documents)}\")  # Debugging statement\n",
        "    print(documents)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer using RAG on retrieved documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG generation\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "\n",
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question\n",
        "    If any document is not relevant, we will set a flag to run web search\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Score each doc\n",
        "    filtered_docs = []\n",
        "    web_search = \"No\"\n",
        "    for d in documents:\n",
        "        score = retrieval_grader.invoke(\n",
        "            {\"question\": question, \"document\": d.page_content}\n",
        "        )\n",
        "        grade = score[\"score\"]\n",
        "        # Document relevant\n",
        "        if grade.lower() == \"yes\":\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            filtered_docs.append(d)\n",
        "        # Document not relevant\n",
        "        else:\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "            # We do not include the document in filtered_docs\n",
        "            # We set a flag to indicate that we want to run web search\n",
        "            web_search = \"Yes\"\n",
        "            continue\n",
        "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
        "\n",
        "\n",
        "def web_search(state):\n",
        "    \"\"\"\n",
        "    Web search based based on the question\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Appended web results to documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---WEB SEARCH---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Web search\n",
        "    docs = web_search_tool.invoke({\"query\": question})\n",
        "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "    web_results = Document(page_content=web_results)\n",
        "    if documents is not None:\n",
        "        documents.append(web_results)\n",
        "    else:\n",
        "        documents = [web_results]\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "\n",
        "### Conditional edge\n",
        "\n",
        "\n",
        "def route_question(state):\n",
        "    \"\"\"\n",
        "    Route question to web search or RAG.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ROUTE QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    print(question)\n",
        "\n",
        "    source = question_router.invoke({\"question\": question})\n",
        "    print(source)\n",
        "    print(source[\"datasource\"])\n",
        "\n",
        "    if source[\"datasource\"] == \"web_search\":\n",
        "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
        "        return \"websearch\"\n",
        "    elif source[\"datasource\"] == \"vectorstore\":\n",
        "        print(\"---ROUTE QUESTION TO RAG---\")\n",
        "        return \"vectorstore\"\n",
        "\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer, or add web search\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Binary decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "    question = state[\"question\"]\n",
        "    web_search = state[\"web_search\"]\n",
        "    filtered_documents = state[\"documents\"]\n",
        "\n",
        "    if web_search == \"Yes\":\n",
        "        # All documents have been filtered check_relevance\n",
        "        # We will re-generate a new query\n",
        "        print(\n",
        "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
        "        )\n",
        "        return \"websearch\"\n",
        "    else:\n",
        "        # We have relevant documents, so generate answer\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\"\n",
        "\n",
        "\n",
        "### Conditional edge\n",
        "\n",
        "\n",
        "def grade_generation_v_documents_and_question(state):\n",
        "    \"\"\"\n",
        "    Determines whether the generation is grounded in the document and answers question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK HALLUCINATIONS---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    generation = state[\"generation\"]\n",
        "\n",
        "    score = hallucination_grader.invoke(\n",
        "        {\"documents\": documents, \"generation\": generation}\n",
        "    )\n",
        "    grade = score[\"score\"]\n",
        "\n",
        "    # Check hallucination\n",
        "    if grade == \"yes\":\n",
        "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "        # Check question-answering\n",
        "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
        "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
        "        grade = score[\"score\"]\n",
        "        if grade == \"yes\":\n",
        "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "            return \"useful\"\n",
        "        else:\n",
        "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
        "            return \"not useful\"\n",
        "    else:\n",
        "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "        return \"not supported\"\n",
        "\n",
        "\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"process_urls_node\", process_urls_node) # process URLs\n",
        "workflow.add_node(\"websearch\", web_search)  # web search\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve from vector store\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "workflow.add_node(\"generate\", generate)  # generate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73f21594-00d4-48a8-ae2e-4e55a010b540",
      "metadata": {
        "id": "73f21594-00d4-48a8-ae2e-4e55a010b540"
      },
      "source": [
        "### Graph Build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9a4b9e4-3ba8-47d6-958c-e5a7112ac6f4",
      "metadata": {
        "id": "d9a4b9e4-3ba8-47d6-958c-e5a7112ac6f4"
      },
      "outputs": [],
      "source": [
        "# Build graph\n",
        "# Set the entry point to process URLs first\n",
        "workflow.set_entry_point(\"process_urls_node\")\n",
        "\n",
        "# Route the question after processing URLs\n",
        "workflow.add_conditional_edges(\n",
        "    \"process_urls_node\",  # Current node\n",
        "    route_question,  # Function to determine the next node\n",
        "    {\n",
        "        \"vectorstore\": \"retrieve\",  # Mapping of condition to next node\n",
        "        \"websearch\": \"websearch\"\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"websearch\": \"websearch\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"websearch\", \"generate\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    grade_generation_v_documents_and_question,\n",
        "    {\n",
        "        \"not supported\": \"generate\",\n",
        "        \"useful\": END,\n",
        "        \"not useful\": \"websearch\",\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "553b3cf2",
      "metadata": {},
      "source": [
        "# Backend Inference Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "352c2de1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing URLs: ['https://arxiv.org/pdf/2403.14403', 'https://arxiv.org/pdf/2401.15884', 'https://arxiv.org/pdf/2310.11511']\n",
            "Number of PDF documents loaded: 59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PDF Retriever created: tags=['FAISS', 'HuggingFaceEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f8271023bb0>\n",
            "Retriever added\n",
            "Retrievers in state: [VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f8271023bb0>)]\n",
            "---ROUTE QUESTION---\n",
            "Tell me about: 1) Adaptive-RAG, 2) Self-Reflective Retrieval-Augmented Generation, and 3) Corrective Retrieval Augmented Generation\n",
            "{'datasource': 'vectorstore'}\n",
            "vectorstore\n",
            "---ROUTE QUESTION TO RAG---\n",
            "'Finished running: process_urls_node:'\n",
            "{'documents': [],\n",
            " 'generation': '',\n",
            " 'question': 'Tell me about: 1) Adaptive-RAG, 2) Self-Reflective '\n",
            "             'Retrieval-Augmented Generation, and 3) Corrective Retrieval '\n",
            "             'Augmented Generation',\n",
            " 'retrievers': [VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f8271023bb0>)],\n",
            " 'urls': ['https://arxiv.org/pdf/2403.14403',\n",
            "          'https://arxiv.org/pdf/2401.15884',\n",
            "          'https://arxiv.org/pdf/2310.11511'],\n",
            " 'web_search': 'No'}\n",
            "''\n",
            "---RETRIEVE---\n",
            "Retrievers in state: [VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f8271023bb0>)]\n",
            "Number of retrievers: 1\n",
            "[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f8271023bb0>)]\n",
            "Documents retrieved: [Document(page_content='Self-CRAG dropped as the retrieval performance\\ndropped, indicating that the generator relied heavily\\non the quality of the retriever. Furthermore, as\\nthe retrieval performance dropped, the generation\\nperformance of Self-CRAG dropped more slightly\\nthan that of Self-RAG. These results imply the\\nsuperiority of Self-CRAG over Self-RAG on en-\\nhancing the robustness to retrieval performance.\\n6 Conclusion\\nThis paper studies the problem where RAG-based\\napproaches are challenged if retrieval goes wrong,\\nthereby exposing inaccurate and misleading knowl-\\nedge to generative LMs. Corrective Retrieval\\nAugmented Generation is proposed to improve the\\nrobustness of generation. Essentially, a lightweight\\nretrieval evaluator is to estimate and trigger three\\nknowledge retrieval actions discriminately. With\\nthe further leverage of web search and optimized\\nknowledge utilization, CRAG has significantly im-\\nproved the ability of automatic self-correction and\\nefficient utilization of retrieved documents. Exper-\\niments extensively demonstrate its adaptability to\\nRAG-based approaches as well as generalizability\\nacross short- and long-form generation tasks.', metadata={'source': 'https://arxiv.org/pdf/2401.15884', 'page': 7}), Document(page_content='threshold, and the answer generation follows.\\n5) Adaptive-RAG. This is our model that adap-\\ntively selects the retrieval-augmented generation\\nstrategy, smoothly oscillating between the non-\\nretrieval, single-step approach, and multi-step ap-\\nproaches4without architectural changes, based on\\nthe query complexity assessed by the classifier.\\n6) Multi-step Approach. This approach (Trivedi\\net al., 2023) is the multi-step retrieval-augmented\\nLLM, which iteratively accesses both the retriever\\nand LLM with interleaved Chain-of-Thought rea-\\nsoning (Wei et al., 2022b) repeatedly until it derives\\nthe solution or reaches the maximum step number.\\n7) Adaptive-RAG w/ Oracle This is an ideal sce-\\nnario of our Adaptive-RAG equipped with an or-\\nacle classifier that perfectly categorizes the query\\ncomplexity.\\nA.3 Implementation Details\\nFor computing resources, we use A100 GPUs\\nwith 80GB memory. In addition, due to the sig-', metadata={'source': 'https://arxiv.org/pdf/2403.14403', 'page': 13}), Document(page_content='Preprint.\\nA S ELF-RAGDETAILS\\nA.1 R EFLECTION TOKENS .\\nDefinitions of reflection tokens. Below, we provide a detailed definition of reflection type and\\noutput tokens. The first three aspects will be provided at each segment level, while the final aspect is\\nonly given at each output level.\\n•Retrieval-on-demand (Retrieve ): Given an input and previous-step generation (if applicable),\\nan LM determines whether the continuation requires factual grounding. Noindicates retrieval\\nis unnecessary as the sequence does not require factual grounding or may not be enhanced by\\nknowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\\nto use evidence , which indicates that a model can continue to use the evidence retrieved\\npreviously. For instance, a passage may contain rich factual information, and thus SELF-RAG\\ngenerates multiple segments based on the passage.\\n•Relevant (ISREL): Retrieved knowledge may not be always relevant to the input. This aspect\\nindicates whether the evidence provides useful information ( Relevant ) or not ( Irrelevant ).\\n•Supported (ISSUP): Attribution is the concept of whether the output is fully supported by', metadata={'source': 'https://arxiv.org/pdf/2310.11511', 'page': 16}), Document(page_content='github.com/starsuzi/Adaptive-RAG .\\n1 Introduction\\nRecent Large Language Models (LLMs) (Brown\\net al., 2020; OpenAI, 2023; Touvron et al., 2023;\\nAnil et al., 2023) have shown overwhelming per-\\nformances across diverse tasks, including question-\\n*Corresponding author\\n0.5 1.0 1.5 2.0 2.5 3.0 3.5\\nTime per Query4748495051Performance (F1)  No Retrieval\\n  Single-step Approach  Adaptive Retrieval  Multi-step Approach   Adaptive-RAG (Ours)Performance vs Time with GPT-3.5Figure 1: QA performance (F1) and efficiency (Time/Query)\\nfor different retrieval-augmented generation approaches. We\\nuse the GPT-3.5-Turbo-Instruct as the base LLM.\\nanswering (QA) (Yang et al., 2018; Kwiatkowski\\net al., 2019). However, they still generate factu-\\nally incorrect answers since their knowledge solely', metadata={'source': 'https://arxiv.org/pdf/2403.14403', 'page': 0})]\n",
            "Total number of documents retrieved: 4\n",
            "[Document(page_content='Self-CRAG dropped as the retrieval performance\\ndropped, indicating that the generator relied heavily\\non the quality of the retriever. Furthermore, as\\nthe retrieval performance dropped, the generation\\nperformance of Self-CRAG dropped more slightly\\nthan that of Self-RAG. These results imply the\\nsuperiority of Self-CRAG over Self-RAG on en-\\nhancing the robustness to retrieval performance.\\n6 Conclusion\\nThis paper studies the problem where RAG-based\\napproaches are challenged if retrieval goes wrong,\\nthereby exposing inaccurate and misleading knowl-\\nedge to generative LMs. Corrective Retrieval\\nAugmented Generation is proposed to improve the\\nrobustness of generation. Essentially, a lightweight\\nretrieval evaluator is to estimate and trigger three\\nknowledge retrieval actions discriminately. With\\nthe further leverage of web search and optimized\\nknowledge utilization, CRAG has significantly im-\\nproved the ability of automatic self-correction and\\nefficient utilization of retrieved documents. Exper-\\niments extensively demonstrate its adaptability to\\nRAG-based approaches as well as generalizability\\nacross short- and long-form generation tasks.', metadata={'source': 'https://arxiv.org/pdf/2401.15884', 'page': 7}), Document(page_content='threshold, and the answer generation follows.\\n5) Adaptive-RAG. This is our model that adap-\\ntively selects the retrieval-augmented generation\\nstrategy, smoothly oscillating between the non-\\nretrieval, single-step approach, and multi-step ap-\\nproaches4without architectural changes, based on\\nthe query complexity assessed by the classifier.\\n6) Multi-step Approach. This approach (Trivedi\\net al., 2023) is the multi-step retrieval-augmented\\nLLM, which iteratively accesses both the retriever\\nand LLM with interleaved Chain-of-Thought rea-\\nsoning (Wei et al., 2022b) repeatedly until it derives\\nthe solution or reaches the maximum step number.\\n7) Adaptive-RAG w/ Oracle This is an ideal sce-\\nnario of our Adaptive-RAG equipped with an or-\\nacle classifier that perfectly categorizes the query\\ncomplexity.\\nA.3 Implementation Details\\nFor computing resources, we use A100 GPUs\\nwith 80GB memory. In addition, due to the sig-', metadata={'source': 'https://arxiv.org/pdf/2403.14403', 'page': 13}), Document(page_content='Preprint.\\nA S ELF-RAGDETAILS\\nA.1 R EFLECTION TOKENS .\\nDefinitions of reflection tokens. Below, we provide a detailed definition of reflection type and\\noutput tokens. The first three aspects will be provided at each segment level, while the final aspect is\\nonly given at each output level.\\n•Retrieval-on-demand (Retrieve ): Given an input and previous-step generation (if applicable),\\nan LM determines whether the continuation requires factual grounding. Noindicates retrieval\\nis unnecessary as the sequence does not require factual grounding or may not be enhanced by\\nknowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\\nto use evidence , which indicates that a model can continue to use the evidence retrieved\\npreviously. For instance, a passage may contain rich factual information, and thus SELF-RAG\\ngenerates multiple segments based on the passage.\\n•Relevant (ISREL): Retrieved knowledge may not be always relevant to the input. This aspect\\nindicates whether the evidence provides useful information ( Relevant ) or not ( Irrelevant ).\\n•Supported (ISSUP): Attribution is the concept of whether the output is fully supported by', metadata={'source': 'https://arxiv.org/pdf/2310.11511', 'page': 16}), Document(page_content='github.com/starsuzi/Adaptive-RAG .\\n1 Introduction\\nRecent Large Language Models (LLMs) (Brown\\net al., 2020; OpenAI, 2023; Touvron et al., 2023;\\nAnil et al., 2023) have shown overwhelming per-\\nformances across diverse tasks, including question-\\n*Corresponding author\\n0.5 1.0 1.5 2.0 2.5 3.0 3.5\\nTime per Query4748495051Performance (F1)  No Retrieval\\n  Single-step Approach  Adaptive Retrieval  Multi-step Approach   Adaptive-RAG (Ours)Performance vs Time with GPT-3.5Figure 1: QA performance (F1) and efficiency (Time/Query)\\nfor different retrieval-augmented generation approaches. We\\nuse the GPT-3.5-Turbo-Instruct as the base LLM.\\nanswering (QA) (Yang et al., 2018; Kwiatkowski\\net al., 2019). However, they still generate factu-\\nally incorrect answers since their knowledge solely', metadata={'source': 'https://arxiv.org/pdf/2403.14403', 'page': 0})]\n",
            "'Finished running: retrieve:'\n",
            "{'documents': [Document(page_content='Self-CRAG dropped as the retrieval performance\\ndropped, indicating that the generator relied heavily\\non the quality of the retriever. Furthermore, as\\nthe retrieval performance dropped, the generation\\nperformance of Self-CRAG dropped more slightly\\nthan that of Self-RAG. These results imply the\\nsuperiority of Self-CRAG over Self-RAG on en-\\nhancing the robustness to retrieval performance.\\n6 Conclusion\\nThis paper studies the problem where RAG-based\\napproaches are challenged if retrieval goes wrong,\\nthereby exposing inaccurate and misleading knowl-\\nedge to generative LMs. Corrective Retrieval\\nAugmented Generation is proposed to improve the\\nrobustness of generation. Essentially, a lightweight\\nretrieval evaluator is to estimate and trigger three\\nknowledge retrieval actions discriminately. With\\nthe further leverage of web search and optimized\\nknowledge utilization, CRAG has significantly im-\\nproved the ability of automatic self-correction and\\nefficient utilization of retrieved documents. Exper-\\niments extensively demonstrate its adaptability to\\nRAG-based approaches as well as generalizability\\nacross short- and long-form generation tasks.', metadata={'source': 'https://arxiv.org/pdf/2401.15884', 'page': 7}),\n",
            "               Document(page_content='threshold, and the answer generation follows.\\n5) Adaptive-RAG. This is our model that adap-\\ntively selects the retrieval-augmented generation\\nstrategy, smoothly oscillating between the non-\\nretrieval, single-step approach, and multi-step ap-\\nproaches4without architectural changes, based on\\nthe query complexity assessed by the classifier.\\n6) Multi-step Approach. This approach (Trivedi\\net al., 2023) is the multi-step retrieval-augmented\\nLLM, which iteratively accesses both the retriever\\nand LLM with interleaved Chain-of-Thought rea-\\nsoning (Wei et al., 2022b) repeatedly until it derives\\nthe solution or reaches the maximum step number.\\n7) Adaptive-RAG w/ Oracle This is an ideal sce-\\nnario of our Adaptive-RAG equipped with an or-\\nacle classifier that perfectly categorizes the query\\ncomplexity.\\nA.3 Implementation Details\\nFor computing resources, we use A100 GPUs\\nwith 80GB memory. In addition, due to the sig-', metadata={'source': 'https://arxiv.org/pdf/2403.14403', 'page': 13}),\n",
            "               Document(page_content='Preprint.\\nA S ELF-RAGDETAILS\\nA.1 R EFLECTION TOKENS .\\nDefinitions of reflection tokens. Below, we provide a detailed definition of reflection type and\\noutput tokens. The first three aspects will be provided at each segment level, while the final aspect is\\nonly given at each output level.\\n•Retrieval-on-demand (Retrieve ): Given an input and previous-step generation (if applicable),\\nan LM determines whether the continuation requires factual grounding. Noindicates retrieval\\nis unnecessary as the sequence does not require factual grounding or may not be enhanced by\\nknowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\\nto use evidence , which indicates that a model can continue to use the evidence retrieved\\npreviously. For instance, a passage may contain rich factual information, and thus SELF-RAG\\ngenerates multiple segments based on the passage.\\n•Relevant (ISREL): Retrieved knowledge may not be always relevant to the input. This aspect\\nindicates whether the evidence provides useful information ( Relevant ) or not ( Irrelevant ).\\n•Supported (ISSUP): Attribution is the concept of whether the output is fully supported by', metadata={'source': 'https://arxiv.org/pdf/2310.11511', 'page': 16}),\n",
            "               Document(page_content='github.com/starsuzi/Adaptive-RAG .\\n1 Introduction\\nRecent Large Language Models (LLMs) (Brown\\net al., 2020; OpenAI, 2023; Touvron et al., 2023;\\nAnil et al., 2023) have shown overwhelming per-\\nformances across diverse tasks, including question-\\n*Corresponding author\\n0.5 1.0 1.5 2.0 2.5 3.0 3.5\\nTime per Query4748495051Performance (F1)  No Retrieval\\n  Single-step Approach  Adaptive Retrieval  Multi-step Approach   Adaptive-RAG (Ours)Performance vs Time with GPT-3.5Figure 1: QA performance (F1) and efficiency (Time/Query)\\nfor different retrieval-augmented generation approaches. We\\nuse the GPT-3.5-Turbo-Instruct as the base LLM.\\nanswering (QA) (Yang et al., 2018; Kwiatkowski\\net al., 2019). However, they still generate factu-\\nally incorrect answers since their knowledge solely', metadata={'source': 'https://arxiv.org/pdf/2403.14403', 'page': 0})],\n",
            " 'question': 'Tell me about: 1) Adaptive-RAG, 2) Self-Reflective '\n",
            "             'Retrieval-Augmented Generation, and 3) Corrective Retrieval '\n",
            "             'Augmented Generation'}\n",
            "'No generation found yet'\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: GENERATE---\n",
            "'Finished running: grade_documents:'\n",
            "{'documents': [Document(page_content='Self-CRAG dropped as the retrieval performance\\ndropped, indicating that the generator relied heavily\\non the quality of the retriever. Furthermore, as\\nthe retrieval performance dropped, the generation\\nperformance of Self-CRAG dropped more slightly\\nthan that of Self-RAG. These results imply the\\nsuperiority of Self-CRAG over Self-RAG on en-\\nhancing the robustness to retrieval performance.\\n6 Conclusion\\nThis paper studies the problem where RAG-based\\napproaches are challenged if retrieval goes wrong,\\nthereby exposing inaccurate and misleading knowl-\\nedge to generative LMs. Corrective Retrieval\\nAugmented Generation is proposed to improve the\\nrobustness of generation. Essentially, a lightweight\\nretrieval evaluator is to estimate and trigger three\\nknowledge retrieval actions discriminately. With\\nthe further leverage of web search and optimized\\nknowledge utilization, CRAG has significantly im-\\nproved the ability of automatic self-correction and\\nefficient utilization of retrieved documents. Exper-\\niments extensively demonstrate its adaptability to\\nRAG-based approaches as well as generalizability\\nacross short- and long-form generation tasks.', metadata={'source': 'https://arxiv.org/pdf/2401.15884', 'page': 7}),\n",
            "               Document(page_content='threshold, and the answer generation follows.\\n5) Adaptive-RAG. This is our model that adap-\\ntively selects the retrieval-augmented generation\\nstrategy, smoothly oscillating between the non-\\nretrieval, single-step approach, and multi-step ap-\\nproaches4without architectural changes, based on\\nthe query complexity assessed by the classifier.\\n6) Multi-step Approach. This approach (Trivedi\\net al., 2023) is the multi-step retrieval-augmented\\nLLM, which iteratively accesses both the retriever\\nand LLM with interleaved Chain-of-Thought rea-\\nsoning (Wei et al., 2022b) repeatedly until it derives\\nthe solution or reaches the maximum step number.\\n7) Adaptive-RAG w/ Oracle This is an ideal sce-\\nnario of our Adaptive-RAG equipped with an or-\\nacle classifier that perfectly categorizes the query\\ncomplexity.\\nA.3 Implementation Details\\nFor computing resources, we use A100 GPUs\\nwith 80GB memory. In addition, due to the sig-', metadata={'source': 'https://arxiv.org/pdf/2403.14403', 'page': 13}),\n",
            "               Document(page_content='Preprint.\\nA S ELF-RAGDETAILS\\nA.1 R EFLECTION TOKENS .\\nDefinitions of reflection tokens. Below, we provide a detailed definition of reflection type and\\noutput tokens. The first three aspects will be provided at each segment level, while the final aspect is\\nonly given at each output level.\\n•Retrieval-on-demand (Retrieve ): Given an input and previous-step generation (if applicable),\\nan LM determines whether the continuation requires factual grounding. Noindicates retrieval\\nis unnecessary as the sequence does not require factual grounding or may not be enhanced by\\nknowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\\nto use evidence , which indicates that a model can continue to use the evidence retrieved\\npreviously. For instance, a passage may contain rich factual information, and thus SELF-RAG\\ngenerates multiple segments based on the passage.\\n•Relevant (ISREL): Retrieved knowledge may not be always relevant to the input. This aspect\\nindicates whether the evidence provides useful information ( Relevant ) or not ( Irrelevant ).\\n•Supported (ISSUP): Attribution is the concept of whether the output is fully supported by', metadata={'source': 'https://arxiv.org/pdf/2310.11511', 'page': 16}),\n",
            "               Document(page_content='github.com/starsuzi/Adaptive-RAG .\\n1 Introduction\\nRecent Large Language Models (LLMs) (Brown\\net al., 2020; OpenAI, 2023; Touvron et al., 2023;\\nAnil et al., 2023) have shown overwhelming per-\\nformances across diverse tasks, including question-\\n*Corresponding author\\n0.5 1.0 1.5 2.0 2.5 3.0 3.5\\nTime per Query4748495051Performance (F1)  No Retrieval\\n  Single-step Approach  Adaptive Retrieval  Multi-step Approach   Adaptive-RAG (Ours)Performance vs Time with GPT-3.5Figure 1: QA performance (F1) and efficiency (Time/Query)\\nfor different retrieval-augmented generation approaches. We\\nuse the GPT-3.5-Turbo-Instruct as the base LLM.\\nanswering (QA) (Yang et al., 2018; Kwiatkowski\\net al., 2019). However, they still generate factu-\\nally incorrect answers since their knowledge solely', metadata={'source': 'https://arxiv.org/pdf/2403.14403', 'page': 0})],\n",
            " 'question': 'Tell me about: 1) Adaptive-RAG, 2) Self-Reflective '\n",
            "             'Retrieval-Augmented Generation, and 3) Corrective Retrieval '\n",
            "             'Augmented Generation',\n",
            " 'web_search': 'No'}\n",
            "'No generation found yet'\n",
            "---GENERATE---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---GRADE GENERATION vs QUESTION---\n",
            "---DECISION: GENERATION ADDRESSES QUESTION---\n",
            "'Finished running: generate:'\n",
            "{'documents': [Document(page_content='Self-CRAG dropped as the retrieval performance\\ndropped, indicating that the generator relied heavily\\non the quality of the retriever. Furthermore, as\\nthe retrieval performance dropped, the generation\\nperformance of Self-CRAG dropped more slightly\\nthan that of Self-RAG. These results imply the\\nsuperiority of Self-CRAG over Self-RAG on en-\\nhancing the robustness to retrieval performance.\\n6 Conclusion\\nThis paper studies the problem where RAG-based\\napproaches are challenged if retrieval goes wrong,\\nthereby exposing inaccurate and misleading knowl-\\nedge to generative LMs. Corrective Retrieval\\nAugmented Generation is proposed to improve the\\nrobustness of generation. Essentially, a lightweight\\nretrieval evaluator is to estimate and trigger three\\nknowledge retrieval actions discriminately. With\\nthe further leverage of web search and optimized\\nknowledge utilization, CRAG has significantly im-\\nproved the ability of automatic self-correction and\\nefficient utilization of retrieved documents. Exper-\\niments extensively demonstrate its adaptability to\\nRAG-based approaches as well as generalizability\\nacross short- and long-form generation tasks.', metadata={'source': 'https://arxiv.org/pdf/2401.15884', 'page': 7}),\n",
            "               Document(page_content='threshold, and the answer generation follows.\\n5) Adaptive-RAG. This is our model that adap-\\ntively selects the retrieval-augmented generation\\nstrategy, smoothly oscillating between the non-\\nretrieval, single-step approach, and multi-step ap-\\nproaches4without architectural changes, based on\\nthe query complexity assessed by the classifier.\\n6) Multi-step Approach. This approach (Trivedi\\net al., 2023) is the multi-step retrieval-augmented\\nLLM, which iteratively accesses both the retriever\\nand LLM with interleaved Chain-of-Thought rea-\\nsoning (Wei et al., 2022b) repeatedly until it derives\\nthe solution or reaches the maximum step number.\\n7) Adaptive-RAG w/ Oracle This is an ideal sce-\\nnario of our Adaptive-RAG equipped with an or-\\nacle classifier that perfectly categorizes the query\\ncomplexity.\\nA.3 Implementation Details\\nFor computing resources, we use A100 GPUs\\nwith 80GB memory. In addition, due to the sig-', metadata={'source': 'https://arxiv.org/pdf/2403.14403', 'page': 13}),\n",
            "               Document(page_content='Preprint.\\nA S ELF-RAGDETAILS\\nA.1 R EFLECTION TOKENS .\\nDefinitions of reflection tokens. Below, we provide a detailed definition of reflection type and\\noutput tokens. The first three aspects will be provided at each segment level, while the final aspect is\\nonly given at each output level.\\n•Retrieval-on-demand (Retrieve ): Given an input and previous-step generation (if applicable),\\nan LM determines whether the continuation requires factual grounding. Noindicates retrieval\\nis unnecessary as the sequence does not require factual grounding or may not be enhanced by\\nknowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\\nto use evidence , which indicates that a model can continue to use the evidence retrieved\\npreviously. For instance, a passage may contain rich factual information, and thus SELF-RAG\\ngenerates multiple segments based on the passage.\\n•Relevant (ISREL): Retrieved knowledge may not be always relevant to the input. This aspect\\nindicates whether the evidence provides useful information ( Relevant ) or not ( Irrelevant ).\\n•Supported (ISSUP): Attribution is the concept of whether the output is fully supported by', metadata={'source': 'https://arxiv.org/pdf/2310.11511', 'page': 16}),\n",
            "               Document(page_content='github.com/starsuzi/Adaptive-RAG .\\n1 Introduction\\nRecent Large Language Models (LLMs) (Brown\\net al., 2020; OpenAI, 2023; Touvron et al., 2023;\\nAnil et al., 2023) have shown overwhelming per-\\nformances across diverse tasks, including question-\\n*Corresponding author\\n0.5 1.0 1.5 2.0 2.5 3.0 3.5\\nTime per Query4748495051Performance (F1)  No Retrieval\\n  Single-step Approach  Adaptive Retrieval  Multi-step Approach   Adaptive-RAG (Ours)Performance vs Time with GPT-3.5Figure 1: QA performance (F1) and efficiency (Time/Query)\\nfor different retrieval-augmented generation approaches. We\\nuse the GPT-3.5-Turbo-Instruct as the base LLM.\\nanswering (QA) (Yang et al., 2018; Kwiatkowski\\net al., 2019). However, they still generate factu-\\nally incorrect answers since their knowledge solely', metadata={'source': 'https://arxiv.org/pdf/2403.14403', 'page': 0})],\n",
            " 'generation': 'Based on the provided context, here are the answers to your '\n",
            "               'questions:\\n'\n",
            "               '\\n'\n",
            "               '1. Adaptive-RAG (Adaptive Retrieval Augmented Generation): '\n",
            "               'This is a model that adaptively selects the '\n",
            "               'retrieval-augmented generation strategy, smoothly oscillating '\n",
            "               'between non-retrieval, single-step approach, and multi-step '\n",
            "               'approaches without architectural changes, based on the query '\n",
            "               'complexity assessed by a classifier.\\n'\n",
            "               '\\n'\n",
            "               '2. Self-Reflective Retrieval-Augmented Generation (Self-RAG): '\n",
            "               'Not explicitly defined in the provided context, but it seems '\n",
            "               'to be an approach that uses reflection tokens to determine '\n",
            "               'whether retrieval is necessary or not.\\n'\n",
            "               '\\n'\n",
            "               '3. Corrective Retrieval Augmented Generation (CRAG): This is a '\n",
            "               'proposed approach to improve the robustness of generation by '\n",
            "               'using a lightweight retrieval evaluator to estimate and '\n",
            "               'trigger three knowledge retrieval actions discriminately, with '\n",
            "               'further leverage of web search and optimized knowledge '\n",
            "               'utilization.',\n",
            " 'question': 'Tell me about: 1) Adaptive-RAG, 2) Self-Reflective '\n",
            "             'Retrieval-Augmented Generation, and 3) Corrective Retrieval '\n",
            "             'Augmented Generation'}\n",
            "('Based on the provided context, here are the answers to your questions:\\n'\n",
            " '\\n'\n",
            " '1. Adaptive-RAG (Adaptive Retrieval Augmented Generation): This is a model '\n",
            " 'that adaptively selects the retrieval-augmented generation strategy, '\n",
            " 'smoothly oscillating between non-retrieval, single-step approach, and '\n",
            " 'multi-step approaches without architectural changes, based on the query '\n",
            " 'complexity assessed by a classifier.\\n'\n",
            " '\\n'\n",
            " '2. Self-Reflective Retrieval-Augmented Generation (Self-RAG): Not explicitly '\n",
            " 'defined in the provided context, but it seems to be an approach that uses '\n",
            " 'reflection tokens to determine whether retrieval is necessary or not.\\n'\n",
            " '\\n'\n",
            " '3. Corrective Retrieval Augmented Generation (CRAG): This is a proposed '\n",
            " 'approach to improve the robustness of generation by using a lightweight '\n",
            " 'retrieval evaluator to estimate and trigger three knowledge retrieval '\n",
            " 'actions discriminately, with further leverage of web search and optimized '\n",
            " 'knowledge utilization.')\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Compile the workflow\n",
        "app = workflow.compile()\n",
        "\n",
        "'''\n",
        "# Test the workflow\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "'''\n",
        "'''\n",
        "urls = [\n",
        "    \"https://www.allrecipes.com/recipe/240290/moms-scalloped-potatoes/\",\n",
        "    \"https://littlesunnykitchen.com/scalloped-potatoes/\",\n",
        "]\n",
        "'''\n",
        "\n",
        "pdf_urls = [\n",
        "    \"https://arxiv.org/pdf/2403.14403\",\n",
        "    \"https://arxiv.org/pdf/2401.15884\",\n",
        "    \"https://arxiv.org/pdf/2310.11511\",\n",
        "]\n",
        "\n",
        "inputs = {\n",
        "    #\"question\": \"What is agent memory?\",\n",
        "    #\"question\": \"How do I make scalloped potatoes?\",\n",
        "    \"question\": \"Tell me about: 1) Adaptive-RAG, 2) Self-Reflective Retrieval-Augmented Generation, and 3) Corrective Retrieval Augmented Generation\",\n",
        "    \"urls\": pdf_urls,\n",
        "    \"retrievers\": [],\n",
        "    \"generation\": \"\",\n",
        "    \"web_search\": \"No\",\n",
        "    \"documents\": [],\n",
        "}\n",
        "\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        pprint(f\"Finished running: {key}:\")\n",
        "        # Print the value dictionary to see all keys\n",
        "        pprint(value)\n",
        "        # Check if 'generation' key exists before printing it\n",
        "        if 'generation' in value:\n",
        "            pprint(value['generation'])\n",
        "        else:\n",
        "            pprint(\"No generation found yet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69uZpDQoVSN7",
      "metadata": {
        "id": "69uZpDQoVSN7"
      },
      "source": [
        "# Frontend Inference Test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BELyaAXlv6Kn",
      "metadata": {
        "id": "BELyaAXlv6Kn"
      },
      "source": [
        "\"https://arxiv.org/pdf/2403.14403\",\n",
        "\"https://arxiv.org/pdf/2401.15884\",\n",
        "\"https://arxiv.org/pdf/2310.11511\",\n",
        "\"https://www.allrecipes.com/recipe/240290/moms-scalloped-potatoes/\",\n",
        "\n",
        "How can I integrate SELF-RAG, Adaptive-RAG, and CRAG together?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd1fb7cd",
      "metadata": {
        "id": "fd1fb7cd"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from pprint import pprint\n",
        "import io\n",
        "import contextlib\n",
        "\n",
        "# Compile the workflow\n",
        "try:\n",
        "    app = workflow.compile()\n",
        "except ValueError as e:\n",
        "    print(f\"Error compiling workflow: {e}\")\n",
        "\n",
        "def process_inputs(question, urls):\n",
        "    # Ensure URLs are split correctly\n",
        "    url_list = [url.strip() for url in urls.replace(\",\", \" \").split()]\n",
        "\n",
        "    # Prepare the input state for the workflow\n",
        "    inputs = {\n",
        "        \"question\": question,\n",
        "        \"urls\": url_list,\n",
        "        \"retrievers\": [],\n",
        "        \"generation\": \"\",\n",
        "        \"web_search\": \"No\",\n",
        "        \"documents\": []\n",
        "    }\n",
        "\n",
        "    final_output = None\n",
        "\n",
        "    # Create a string buffer to capture the printed output\n",
        "    with io.StringIO() as buf, contextlib.redirect_stdout(buf):\n",
        "        try:\n",
        "            for output in app.stream(inputs):\n",
        "                for key, value in output.items():\n",
        "                    pprint(f\"Finished running: {key}:\")\n",
        "                    final_output = value.get(\"generation\", \"No generation found\")\n",
        "\n",
        "            # Capture the printed output\n",
        "            pprint(final_output)\n",
        "            output_string = buf.getvalue()\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing inputs: {e}\")\n",
        "            output_string = str(e)\n",
        "\n",
        "    return output_string\n",
        "\n",
        "# Define the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=process_inputs,\n",
        "    inputs=[\"text\", \"text\"],\n",
        "    outputs=\"text\",\n",
        "    title=\"Advanced RAG UI\",\n",
        "    description=(\"Ask a question and enter relevant URLs, each URL separated by a comma.\"\n",
        "                \" Outputs an answer using a combination of vector store retrieval and web search\")\n",
        ")\n",
        "\n",
        "iface.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
